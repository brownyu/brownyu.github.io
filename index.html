<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DSA 306</title>
</head>
<style>
    h2.title {
        text-align: center;
    }
    small {
    display: inline-block;
    }
</style>
<body>
    <h2 class="title"> Here is the DSA 306 Big data analytics group project to predict the student dropout</h2>
    <h4>Project Report: Predict Student's Dropout and academic success</h4>
    <h4>Our group is able to model a logistic regression model in Spark pipeline to classify the student in terms of drop out with a ROC value of  0.9494767 on test data set. </h4>
    <p>
    <a href="https://github.com/brownyu/brownyu.github.io">R code and dataset is available at this github page</a>
    </p>
    <div>
        <p>Group members are</p>
            <li>Brian Lim</li>
            <li>Eugene Wong </li>
            <li>Tan Gong Chuen </li>  
            <li>Zhao Siyu</li>
    </div>

    <p> Our project could be viewed in several parts</p>
    <ol>
        <li>Introduction</li>
        <li>Data set overview</li>
        <li>Data wrangling in Spark</li>
        <li>EDA</li>
        <li>Modelling in Spark</li>
        <li>Conclusion</li>
    </ol> 
    <h4>1.Introduction</h4>
    <p>The dataset was donated by a group of researchers from Portugal who were studying dropout rates and academic performance among undergraduate students from a higher education institution. 
        The dataset includes a wide range of variables like the course enrolled, the student demographics, and other social-economic factors. 
        We wanted to find a model that can perform a decent job at estimating the likelihood that a student will drop out. As such, the model can provide the school a better idea of which students of theirs are more prone to dropout. With the information at hand, the school may carry out early intervention and support for vulnerable students in order to curb dropout rates.  
    </p>
    <h4>2.Data set overview</h4>
    <p>The data is based on a higher education institution in Portugal and obtained through SATDAP – Support System for the Digital Transformation of Public Administration (SAMA2020). The data has been preprocessed to handle data from anomalies, unexplainable outliers and missing values, resulting in 4424 instances. The data also contains 37 variables. The variables include 18 continuous/discrete variables and 19 categorical variables.</p>

    <h4>3.Data Wrangling in Spark</h4>
    <p>The parquet file was first loaded into RStudio using spark_read_parquet(). The data came in extremely messy without the numbers neatly divided into columns</p>
    <p>We used filter() to filter out the “Enrolled” observations from the target variable as it is undetermined whether these students will drop out or not. After which, we used mutate() and as.numeric() to create a new column (isDropout) where 1 is assigned to those who dropped out and 0 to those who graduated.</p>

    <h4>4.EDA</h4>
    <h5>Distribution of some independent variables</h5>
    <ol>
        <li>Marital Status</li>
        <div>
            <p>Category 6 (Legally Separated) seems to be of significance as it has a high proportion of dropout of 0.8. Although a large percentage of observations fall under the “Single” category which has a 0.37 proportion of dropouts, it could still be a significant feature. </p>
        <img src="./images/maritalstatus.jpg"  class="inline" >
        </div>
        <li>Application Mode</li>
        <div>
            <p>Application modes 7 (Holders of other higher courses) and 39 (Over 23 years old) seem to be of significance with dropout proportions of 0.64 and 0.66 respectively. 
            </p>
        <img src="./images/applicationmode.jpg"  class="inline" >
        </div>
        <li>Course</li>
        <div>
            <p> Courses 33, 9119, 9130, 9853 and 9991 have dropout proportions of 0.6 and above. As such, we have also identified it to be a significant feature</p>
        <img src="./images/course.jpg"  class="inline" >
        </div>
        <li> Debtor</li>
        <div>
            <p>A debtor seems to be of significance as it has a high dropout proportion of 0.76 with 413 counts of observations.</p>
        <img src="./images/debtor.jpg"  class="inline" >
        </div>
        <li>Tuition Fees Up To Date</li>
        <div>
            <p> Tuition fees not up to date seems to be of significance as it has a high dropout proportion of 0.94 with 486 counts of observations</p>
        <img src="./images/tuitionfees.jpg"  class="inline" >
        </div>
        <li>Age At Enrollment</li>
        <div>
            <p> There seems to be a positive relationship between the dropout proportions and age at enrollment</p>
        <img src="./images/age.jpg"  class="inline" >
        </div>
    </ol> 
    <h5>Balance of dataset</h5>
    <p>The table]shows 60% of data have a dropout 0 and 40% of the data has a dropout 1. Even though the data set is not evenly split between two classes, a 60-40 split is still considered good enough for our model to do the classification task. </p>
    <img src="./images/balance.jpg"  class="inline" >
    <h4>5.Modelling in Spark </h4>
    <h5>Feature Transform </h5>
    <p>we have to convert our variables to a numeric form for the ml_logistic_regression() to process. However, since we have multiple levels in some categorical variables, for example Course, we used ft_string_indexer() to first index the string column to an ML column of label indices, ordered by label frequencies. Afterwards, we used ft_one_hot_encoder() to map the column of label indices to a column of binary vectors for each categorical variable.</p>
    <h5>Pipeline</h5>
    <p> we made use of a pipeline which contains ft_vector_assembler() and ft_standard_scaler() to scale them. Subsequently, we would combine standardised variables with categorical variables using vector assembler again, which are passed into ml_logistic_regression() to regress isDropout against all independent variables. 
    </p>
    <img src="./images/pipeline.JPG"  class="inline" >
    <h5>Logistic Regression Modelling</h5>
    <p>Before modelling, we split the data into a training and testing set using a 8:2 ratio with seed = 888. Then, we fitted the pipeline onto the training set and used it to transform and get predictions on the testing set. The measure that we used to compare models will be the “area under the ROC curve” </p>
    <h5>Model with all variables</h5>
    <p>We first plugged all the variables into a logistic regression and we will use this model as a benchmark model for comparison. The benchmark model has an area under the ROC curve of 0.9295625.</p>
    <h5>Model with all variables with Elastic net regularisation</h5>
    <p> To find the best hyperparameter value, α and lambda, for the elastic net regularisation, we made use of ml_cross_validator() to perform 10 folds CV and tested a range of values elastic_net_param = c(0.25,0.4,0.5,0.6,0.75,0.8,0.85), and reg_param = c(seq(0.001,0.01,by = 0.001)) on the training set. We then evaluate and select the best combination of hyperparameter values with the highest ROC. From our results, the best combination is elastic_net_param = 0.6, with reg_param = 0.004. The area under ROC for this selected elastic net regularised logistic regression model is 0.9441253 when used to predict on the testing data. </p>
    <h5>Model with selected variables</h5>
    <p> Further data wrangling in this model by summation various curricular units from two semesters together. Cross-validation and elastic net regularization is also used in this model, to tune the best parameters. The model has an area under ROC of 0.9494767 in testing data </p>
    <h4>6.Conclusion</h4>
    <img src="./images/precision_recall.JPG"  class="inline" >
    <p>By comparing these three models, we think the last model is the best model with highest ROC value</p>

</body>
</html>